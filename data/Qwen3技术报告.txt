# Qwen3技术报告
Qwen团队
https://huggingface.co/Qwen
https://modelscope.cn/organization/qwen
https://github.com/QwenLM/Qwen3

## 摘要
在这项工作中，我们介绍了Qwen模型家族的最新版本Qwen3。Qwen3包含一系列旨在提升性能、效率和多语言能力的大型语言模型（LLM）。Qwen3系列包括密集型和混合专家（MoE）架构的模型，参数规模从6亿到2350亿不等。Qwen3的一个关键创新是将思考模式（用于复杂的多步推理）和非思考模式（用于快速的上下文驱动响应）集成到一个统一框架中。这消除了在不同模型之间切换的需要，例如经过聊天优化的模型（如GPT-4o）和专用推理模型（如QwQ32B），并支持根据用户查询或聊天模板动态切换模式。同时，Qwen3引入了思考预算机制，允许用户在推理过程中自适应分配计算资源，从而根据任务复杂度平衡延迟和性能。此外，通过利用旗舰模型的知识，我们显著减少了构建小规模模型所需的计算资源，同时确保它们具有极具竞争力的性能。实证评估表明，Qwen3在各种基准测试中均取得了最先进的结果，包括代码生成、数学推理、代理任务等，可与更大的MoE模型和专有模型相媲美。与前身Qwen2.5相比，Qwen3将多语言支持从29种扩展到119种语言和方言，通过增强的跨语言理解和生成能力提高了全球可访问性。为促进可重复性和社区驱动的研究与开发，所有Qwen3模型均在Apache 2.0下公开提供。

arXiv:2505.09388v1 [cs.CL] 2025年5月14日

## 1 引言
追求通用人工智能（AGI）或人工超级智能（ASI）长期以来一直是人类的目标。大型基础模型的最新进展，例如GPT-4o（OpenAI，2024）、Claude 3.7（Anthropic，2025）、Gemini 2.5（DeepMind，2025）、DeepSeek-V3（Liu等人，2024a）、Llama-4（Meta-AI，2025）和Qwen2.5（Yang等人，2024b），已在朝着这一目标的方向上展示出显著进展。这些模型在跨越数万亿token、涵盖各种领域和任务的大规模数据集上进行训练，有效地将人类知识和能力提炼到其参数中。此外，通过强化学习优化的推理模型的最新发展，凸显了基础模型在提升推理时缩放和实现更高智能水平方面的潜力，例如o3（OpenAI，2025）、DeepSeek-R1（Guo等人，2025）。尽管大多数最先进的模型仍然是专有的，但开源社区的快速发展已大幅缩小了开源模型与闭源模型之间的性能差距。值得注意的是，越来越多的顶级模型（Meta-AI，2025；Liu等人，2024a；Guo等人，2025；Yang等人，2024b）现在以开源形式发布，促进了人工智能领域更广泛的研究和创新。

在这项工作中，我们介绍了我们基础模型家族Qwen的最新系列Qwen3。Qwen3是一组开源权重的大型语言模型（LLM），在各种各样的任务和领域中均取得了最先进的性能。我们发布了密集型和混合专家（MoE）模型，参数数量从6亿到2350亿不等，以满足不同下游应用的需求。值得注意的是，旗舰模型Qwen3-235B-A22B是一个MoE模型，总参数为2350亿，每个token激活220亿参数。这种设计确保了高性能和高效推理。

Qwen3引入了几项关键进展以增强其功能和可用性。首先，它将两种不同的操作模式——思考模式和非思考模式——集成到单个模型中。这使用户无需在不同模型之间切换，例如从Qwen2.5切换到QwQ（Qwen团队，2024）。这种灵活性确保开发人员和用户能够有效地使模型行为适应特定任务。此外，Qwen3纳入了思考预算，为用户提供了对模型在任务执行期间应用的推理努力程度的细粒度控制。此功能对于计算资源和性能的优化至关重要，可使模型的思考行为适应现实应用中的不同复杂度。此外，Qwen3已在覆盖多达119种语言和方言的36万亿token上进行了预训练，有效增强了其多语言能力。这种扩展的语言支持放大了其在全球用例和国际应用中部署的潜力。这些进展共同使Qwen3成为一个前沿的开源大型语言模型家族，能够有效解决跨各种领域和语言的复杂任务。

Qwen3的预训练过程利用了一个由约36万亿token组成的大规模数据集，精心策划以确保语言和领域的多样性。为了高效扩展训练数据，我们采用了多模态方法：对Qwen2.5-VL（Bai等人，2025）进行微调，以从大量PDF文档中提取文本。我们还使用特定领域模型生成合成数据：Qwen2.5-Math（Yang等人，2024c）用于数学内容，Qwen2.5-Coder（Hui等人，2024）用于代码相关数据。预训练过程遵循三阶段策略。在第一阶段，模型在约30万亿token上进行训练，以构建强大的通用知识基础。在第二阶段，进一步在知识密集型数据上训练，以增强在科学、技术、工程和数学（STEM）及编码等领域的推理能力。最后，在第三阶段，在长上下文数据上训练，将最大上下文长度从4096增加到32768 token。

为了更好地使基础模型与人类偏好和下游应用保持一致，我们采用了多阶段后训练方法，赋予思考（推理）和非思考模式能力。在前两个阶段，我们专注于通过长思维链（CoT）冷启动微调和专注于数学和编码任务的强化学习来开发强大的推理能力。在最后两个阶段，我们将包含和不包含推理路径的数据合并到统一数据集中进行进一步微调，使模型能够有效处理两种类型的输入，然后应用通用领域强化学习来提高跨广泛下游任务的性能。对于较小的模型，我们使用强到弱蒸馏，利用来自较大模型的离线和在线知识转移来增强其能力。来自先进教师模型的蒸馏在性能和训练效率上显著优于强化学习。

我们在涵盖多个任务和领域的综合基准测试集上评估了模型的预训练和后训练版本。实验结果表明，我们的基础预训练模型取得了最先进的性能。后训练模型，无论是在思考还是非思考模式下，均与领先的专有模型和大型混合专家（MoE）模型（如o1、o3-mini和DeepSeek-V3）具有竞争力。值得注意的是，我们的模型在编码、数学和代理相关任务中表现出色。例如，旗舰模型Qwen3-235B-A22B在AIME’24上得分为85.7，在AIME’25上得分为81.5（AIME，2025），在LiveCodeBench v5上得分为70.7（Jain等人，2024），在CodeForces上得分为2056，在BFCL v3上得分为70.8（Yan等人，2024）。此外，Qwen3系列中的其他模型相对于其规模也显示出强大的性能。此外，我们观察到，为思考token增加思考预算会导致模型在各种任务上的性能持续提升。

在以下部分中，我们描述了模型架构的设计，提供了训练过程的详细信息，呈现了预训练和后训练模型的实验结果，最后通过总结关键发现和概述未来研究的潜在方向来结束本技术报告。

## 2 架构
Qwen3系列包括6个密集型模型，即Qwen3-0.6B、Qwen3-1.7B、Qwen3-4B、Qwen3-8B、Qwen3-14B和Qwen3-32B，以及2个MoE模型，Qwen3-30B-A3B和Qwen3-235B-A22B。旗舰模型Qwen3-235B-A22B总参数为2350亿，其中激活参数为220亿。下面，我们详细阐述Qwen3模型的架构。

Qwen3密集型模型的架构与Qwen2.5（Yang等人，2024b）类似，包括使用分组查询注意力（GQA，Ainslie等人，2023）、SwiGLU（Dauphin等人，2017）、旋转位置嵌入（RoPE，Su等人，2024）和带预归一化的RMSNorm（Jiang等人，2023）。此外，我们移除了Qwen2中使用的QKV偏差（Yang等人，2024a），并在注意力机制中引入了QK-Norm（Dehghani等人，2023），以确保Qwen3的稳定训练。模型架构的关键信息见表1。

|模型|层数|头数（Q/KV）|词嵌入绑定|上下文长度|
|----|----|----|----|----|
|Qwen3-0.6B|28|16/8|是|32K|
|Qwen3-1.7B|36|16/8|是|128K|
|Qwen3-4B|36|32/8|否|128K|
|Qwen3-8B|28|32/8|否|32K|
|Qwen3-14B|40|40/8|否|128K|
|Qwen3-32B|64|64/8|否|128K|

Qwen3 MoE模型与Qwen3密集型模型共享相同的基础架构。模型架构的关键信息见表2。我们遵循Qwen2.5-MoE（Yang等人，2024b）并实现了细粒度专家分割（Dai等人，2024）。Qwen3 MoE模型总共有128个专家，每个token激活8个专家。与Qwen2.5-MoE不同，Qwen3-MoE设计中不包含共享专家。此外，我们采用了全局批处理负载平衡损失（Qiu等人，2025）来促进专家专业化。这些架构和训练创新已在下游任务中带来了模型性能的显著提升。

|模型|层数|头数（Q/KV）|专家数量（总计/激活）|上下文长度|
|----|----|----|----|----|
|Qwen3-30B-A3B|48|32/4|128/8|128K|
|Qwen3-235B-A22B|94|64/4|128/8|128K|

Qwen3模型使用Qwen的分词器（Bai等人，2023），该分词器实现了字节级字节对编码（BBPE，Brown等人，2020；Wang等人，2020；Sennrich等人，2016），词汇量为151,669。

## 3 预训练
在本节中，我们描述预训练数据的构建、预训练方法的细节，并呈现基于标准基准评估基础模型的实验结果。

### 3.1 预训练数据
与Qwen2.5（Yang等人，2024b）相比，我们显著扩展了训练数据的规模和多样性。具体而言，我们收集了两倍的预训练token，覆盖了三倍的语言。所有Qwen3模型都在一个由119种语言和方言组成的大型多样化数据集上进行训练，总共有36万亿token。该数据集包括各种领域的高质量内容，如编码、STEM（科学、技术、工程和数学）、推理任务、书籍、多语言文本和合成数据。

为了进一步扩展预训练数据语料库，我们首先使用Qwen2.5-VL模型（Bai等人，2025）对大量类似PDF的文档进行文本识别。然后使用Qwen2.5模型（Yang等人，2024b）对识别的文本进行精炼，这有助于提高其质量。通过这个两步过程，我们能够获得额外的数万亿高质量文本token。此外，我们使用Qwen2.5（Yang等人，2024b）、Qwen2.5-Math（Yang等人，2024c）和Qwen2.5-Coder（Hui等人，2024）模型来合成不同格式的数万亿文本token，包括教科书、问答、指令和代码片段，涵盖数十个领域。最后，我们通过纳入额外的多语言数据和引入更多语言来进一步扩展预训练语料库。与Qwen2.5中使用的预训练数据相比，支持的语言数量已从29种显著增加到119种，增强了模型的语言覆盖范围和跨语言能力。

我们开发了一个多语言数据注释系统，旨在提高训练数据的质量和多样性。该系统已应用于我们的大规模预训练数据集，对超过30万亿token进行了多个维度的注释，如教育价值、领域、域和安全性。这些详细注释支持更有效的数据过滤和组合。与之前在数据源或领域级别优化数据混合的研究（Xie等人，2023；Fan等人，2023；Liu等人，2024b）不同，我们的方法通过对具有细粒度数据标签的小型代理模型进行广泛的消融实验，在实例级别优化数据混合。

### 3.2 预训练阶段
Qwen3模型通过三阶段过程进行预训练：

（1）通用阶段（S1）：在第一个预训练阶段，所有Qwen3模型都在超过30万亿token上使用4096token的序列长度进行训练。在这个阶段，模型已经在语言能力和一般世界知识上完成了全面预训练，训练数据覆盖119种语言和方言。

（2）推理阶段（S2）：为了进一步提高推理能力，我们通过增加STEM、编码、推理和合成数据的比例来优化此阶段的预训练语料库。模型使用约5万亿更高质量的token进行进一步预训练，序列长度为4096token。我们还在此阶段加速学习率衰减。

（3）长上下文阶段：在最后的预训练阶段，我们收集高质量的长上下文语料库来扩展Qwen3模型的上下文长度。所有模型都在数百亿token上进行预训练，序列长度为32768token。长上下文语料库包括75%长度在16384到32768token之间的文本，以及25%长度在4096到16384之间的文本。遵循Qwen2.5（Yang等人，2024b），我们使用ABF技术（Xiong等人，2023）将RoPE的基础频率从10,000增加到1,000,000。同时，我们引入了YARN（Peng等人，2023）和双块注意力（DCA，An等人，2024），以在推理期间实现序列长度容量的四倍增加。

与Qwen2.5（Yang等人，2024b）类似，我们基于上述三个预训练阶段开发了用于优化超参数（如学习率调度器和批量大小）预测的缩放定律。通过广泛的实验，我们系统地研究了模型架构、训练数据、训练阶段和最佳训练超参数之间的关系。最后，我们为每个密集型或MoE模型设置了预测的最佳学习率和批量大小策略。

### 3.3 预训练评估
我们对Qwen3系列的基础语言模型进行了全面评估。基础模型的评估主要集中在它们在通用知识、推理、数学、科学知识、编码和多语言能力方面的表现。预训练基础模型的评估数据集包括15个基准：

通用任务：MMLU（Hendrycks等人，2021a）（5-shot）、MMLU-Pro（Wang等人，2024）（5-shot，CoT）、MMLU-redux（Gema等人，2024）（5-shot）、BBH（Suzgun等人，2023）（3-shot，CoT）、SuperGPQA（Du等人，2025）（5-shot，CoT）。

数学与STEM任务：GPQA（Rein等人，2023）（5-shot，CoT）、GSM8K（Cobbe等人，2021）（4-shot，CoT）、MATH（Hendrycks等人，2021b）（4-shot，CoT）。

编码任务：EvalPlus（Liu等人，2023a）（0-shot）（HumanEval（Chen等人，2021）、MBPP（Austin等人，2021）、Humaneval+、MBPP+（Liu等人，2023a）的平均值）、MultiPL-E（Cassano等人，2023）（0-shot）（Python、C++、JAVA、PHP、TypeScript、C#、Bash、JavaScript）、MBPP-3shot（Austin等人，2021）、CRUX-O of CRUXEval（1-shot）（Gu等人，2024）。

多语言任务：MGSM（Shi等人，2023）（8-shot，CoT）、MMMLU（OpenAI，2024）（5-shot）、INCLUDE（Romanou等人，2024）（5-shot）。

对于基础模型基线，我们将Qwen3系列基础模型与Qwen2.5基础模型（Yang等人，2024b）和其他领先的开源基础模型进行比较，包括DeepSeek-V3 Base（Liu等人，2024a）、Gemma-3（Team等人，2025）、Llama-3（Dubey等人，2024）和Llama-4（Meta-AI，2025）系列基础模型，按参数规模进行比较。所有模型都使用相同的评估管道和广泛使用的评估设置进行评估，以确保公平比较。

### 评估结果总结
基于整体评估结果，我们强调Qwen3基础模型的一些关键结论。

（1）与之前开源的SOTA密集型和MoE基础模型（如DeepSeekV3 Base、Llama-4-Maverick Base和Qwen2.5-72B-Base）相比，Qwen3-235B-A22B-Base在大多数任务中以显著更少的总参数或激活参数超越了这些模型。

（2）对于Qwen3 MoE基础模型，我们的实验结果表明：（a）使用相同的预训练数据，Qwen3 MoE基础模型仅使用1/5的激活参数即可实现与Qwen3密集型基础模型相似的性能。（b）由于Qwen3 MoE架构的改进、训练token的规模扩大和更先进的训练策略，Qwen3 MoE基础模型可以用少于1/2的激活参数和更少的总参数超越Qwen2.5 MoE基础模型。（c）即使使用Qwen2.5密集型基础模型1/10的激活参数，Qwen3 MoE基础模型也能实现可比的性能，这为我们在推理和训练成本方面带来了显著优势。

（3）Qwen3密集型基础模型的整体性能与更高参数规模的Qwen2.5基础模型相当。例如，Qwen3-1.7B/4B/8B/14B/32B-Base分别实现了与Qwen2.5-3B/7B/14B/32B/72B-Base相当的性能。特别是在STEM、编码和推理基准测试中，Qwen3密集型基础模型的性能甚至超越了更高参数规模的Qwen2.5基础模型。

详细结果如下。

#### Qwen3-235B-A22B-Base
我们将Qwen3-235B-A22B-Base与我们之前类似规模的MoE模型Qwen2.5-Plus-Base（Yang等人，2024b）和其他领先的开源基础模型进行比较：Llama-4-Maverick（Meta-AI，2025）、Qwen2.5-72B-Base（Yang等人，2024b）、DeepSeek-V3 Base（Liu等人，2024a）。从表3的结果来看，Qwen3-235B-A22B-Base模型在大多数评估基准上获得了最高的性能分数。我们进一步将Qwen3-235B-A22B-Base与其他基线分别进行详细分析。

（1）与最近开源的参数约为其两倍的模型Llama-4-Maverick-Base相比，Qwen3-235B-A22B-Base在大多数基准上仍然表现更好。

（2）与之前最先进的开源模型DeepSeek-V3-Base相比，Qwen3-235B-A22B-Base在15个评估基准中的14个上超越了DeepSeek-V3-Base，仅使用约1/3的总参数和2/3的激活参数，展示了我们模型的强大和成本效益。

（3）与我们之前类似规模的MoE模型Qwen2.5-Plus相比，Qwen3-235B-A22B-Base以更少的参数和激活参数显著超越了它，这表明Qwen3在预训练数据、训练策略和模型架构方面的显著优势。

（4）与我们之前的旗舰开源密集型模型Qwen2.5-72B-Base相比，Qwen3-235B-A22B-Base在所有基准上都超越了后者，并且使用的激活参数不到1/3。同时，由于模型架构的优势，Qwen3-235B-A22B-Base在每万亿token上的推理成本和训练成本都比Qwen2.5-72B-Base低得多。

| |Base|Base|Base|Base|Qwen2.5-72B|Qwen2.5-Plus|Llama-4-Maverick|DeepSeek-V3|Qwen3-235B-A22B|
|----|----|----|----|----|----|----|----|----|----|
|架构|密集型|MoE|MoE|MoE|MoE|
|总参数数量|72B|271B|402B|671B|235B|
|激活参数数量|72B|37B|17B|37B|22B|
|通用任务|||
|MMLU|86.06|85.02|85.16|87.19|87.81|
|MMLU-Redux|83.91|82.69|84.05|86.14|87.40|
|MMLU-Pro|58.07|63.52|63.91|59.84|68.18|
|SuperGPQA|36.20|37.18|40.85|41.53|44.06|
|BBH|86.30|85.60|83.62|86.22|88.87|
|数学与STEM任务|||
|GPQA|45.88|41.92|43.94|41.92|47.47|
|GSM8K|91.50|91.89|87.72|87.57|94.39|
|MATH|62.12|62.78|63.32|62.62|71.84|
|编码任务|||
|EvalPlus|65.93|61.43|68.38|63.75|77.60|
|MultiPL-E|58.70|62.16|57.28|62.26|65.94|
|MBPP|76.00|74.60|75.40|74.20|81.40|
|CRUX-O|66.20|68.50|77.00|76.60|79.00|
|多语言任务|||
|MGSM|82.40|82.21|79.69|82.68|83.53|
|MMMLU|84.40|83.49|83.09|85.88|86.70|
|INCLUDE|69.05|66.97|73.47|75.17|73.46|

#### Qwen3-32B-Base
Qwen3-32B-Base是我们Qwen3系列中最大的密集型模型。我们将其与类似规模的基线进行比较，包括Gemma-3-27B（Team等人，2025）和Qwen2.5-32B（Yang等人，2024b）。此外，我们引入了两个强大的基线：最近开源的MoE模型Llama4-Scout，其参数是Qwen3-32B-Base的三倍，但激活参数是其一半；以及我们之前的旗舰开源密集型模型Qwen2.5-72B-Base，其参数是Qwen3-32B-Base的两倍多。结果如表4所示，支持三个关键结论：

（1）与类似规模的模型相比，Qwen3-32B-Base在大多数基准上超越了Qwen2.5-32B-Base和Gemma-3-27B Base。值得注意的是，Qwen3-32B-Base在MMLUPro上得分为65.54，在SuperGPQA上得分为39.78，显著超越了其前身Qwen2.5-32B-Base。此外，Qwen3-32B-Base在编码基准分数上显著高于所有基线模型。

（2）令人惊讶的是，我们发现Qwen3-32B-Base与Qwen2.5-72B-Base相比取得了具有竞争力的结果。尽管Qwen3-32B-Base的参数不到Qwen2.5-72B-Base的一半，但在15个评估基准中的10个上超越了Qwen2.5-72B-Base。在编码、数学和推理基准上，Qwen3-32B-Base具有显著优势。

（3）与Llama-4-Scout-Base相比，Qwen3-32B-Base在所有15个基准上都显著超越了它，Qwen3-32B-Base的参数仅为Llama-4-Scout-Base的三分之一，但激活参数是其两倍。

| |Qwen2.5-32B Base|Qwen2.5-72B Base|Gemma-3-27B Base|Llama-4-Scout Base|Qwen3-32B Base|
|----|----|----|----|----|----|
|架构|密集型|密集型|密集型|MoE|密集型|
|总参数数量|32B|72B|27B|109B|32B|
|激活参数数量|32B|72B|27B|17B|32B|
|通用任务|||
|MMLU|83.32|86.06|78.69|78.27|83.61|
|MMLU-Redux|81.97|83.91|76.53|71.09|83.41|
|MMLU-Pro|55.10|58.07|52.88|56.13|65.54|
|SuperGPQA|33.55|36.20|29.87|26.51|39.78|
|BBH|84.48|86.30|79.95|82.40|87.38|
|数学与STEM任务|||
|GPQA|47.97|45.88|26.26|40.40|49.49|
|GSM8K|92.87|91.50|81.20|85.37|93.40|
|MATH|57.70|62.12|51.78|51.66|61.62|
|编码任务|||
|EvalPlus|66.25|65.93|55.78|59.90|72.05|
|MultiPL-E|58.30|58.70|45.03|47.38|67.06|
|MBPP|73.60|76.00|68.40|68.60|78.20|
|CRUX-O|67.80|66.20|60.00|61.90|72.50|
|多语言任务|||
|MGSM|78.12|82.40|73.74|79.93|83.06|
|MMMLU|82.40|84.40|77.62|74.83|83.83|
|INCLUDE|64.35|69.05|68.94|68.09|67.87|

#### Qwen3-14B-Base & Qwen3-30B-A3B-Base
Qwen3-14B-Base和Qwen3-30BA3B-Base的评估是与类似规模的基线进行比较，包括Gemma-3-12B Base、Qwen2.5-14B Base。同样，我们还引入了两个强大的基线：（1）Qwen2.5-Turbo（Yang等人，2024b），其参数为42B，激活参数为6B。请注意，其激活参数是Qwen3-30B-A3B-Base的两倍。（2）Qwen2.5-32B-Base，其激活参数是Qwen3-30B-A3B的11倍，是Qwen3-14B的两倍多。结果如表5所示，我们可以得出以下结论。

（1）与类似规模的模型相比，Qwen3-14B-Base在所有15个基准上显著优于Qwen2.5-14B-Base和Gemma-3-12B-Base。

（2）同样，Qwen3-14B-Base与参数不到一半的Qwen2.5-32B-Base相比也取得了非常有竞争力的结果。

（3）Qwen3-30B-A3B仅使用1/5的非嵌入激活参数，在所有任务上显著超越了Qwen2.5-14B-Base，并实现了与Qwen3-14B-Base和Qwen2.5-32B-Base相当的性能，这为我们在推理和训练成本方面带来了显著优势。

#### Qwen3-8B / 4B / 1.7B / 0.6B-Base
对于边缘侧模型，我们将类似规模的Qwen2.5、Llama-3和Gemma-3基础模型作为基线。结果可见于表6、表7和表8。所有Qwen3 8B / 4B / 1.7B / 0.6B-Base模型在几乎所有基准上都继续保持强大的性能。值得注意的是，Qwen3-8B / 4B / 1.7B-Base模型甚至在超过一半的基准上超越了更大尺寸的Qwen2.5-14B / 7B / 3B Base模型，尤其是在STEM相关和编码基准上，反映了Qwen3模型的显著改进。

## 4 后训练

### 4.1 长CoT冷启动
我们首先策划了一个涵盖广泛类别的综合数据集，包括数学、代码、逻辑推理和一般STEM问题。数据集中的每个问题都与经过验证的参考答案或基于代码的测试用例配对。这个数据集作为长思维链（长-CoT）训练“冷启动”阶段的基础。

数据集构建涉及严格的两阶段过滤过程：查询过滤和响应过滤。在查询过滤阶段，我们使用Qwen2.5-72B-Instruct来识别和删除不易验证的查询。这包括包含多个子问题或请求一般文本生成的查询。此外，我们排除了Qwen2.5-72B-Instruct无需使用CoT推理即可正确回答的查询。这有助于防止模型依赖表面猜测，并确保仅包含需要更深层次推理的复杂问题。此外，我们使用Qwen2.5-72B-Instruct对每个查询的领域进行注释，以保持数据集中的领域表示平衡。

在保留验证查询集后，我们使用QwQ-32B（Qwen团队，2025）为每个剩余查询生成N个候选响应。当QwQ-32B始终无法生成正确解决方案时，人类注释者会手动评估响应的准确性。对于Pass@N为正的查询，应用更严格的过滤标准以删除以下响应：（1）产生不正确的最终答案，（2）包含大量重复，（3）明显表明没有充分推理的猜测，（4）思考和总结内容之间存在不一致，（5）涉及不适当的语言混合或风格转变，或（6）涉嫌与潜在验证集项目过于相似。随后，将精炼数据集的精心选择子集用于推理模式的初始冷启动训练。此阶段的目标是在模型中灌输基础推理模式，而不过度强调即时推理性能。这种方法确保模型的潜力不受限制，允许在后续强化学习（RL）阶段有更大的灵活性和改进空间。为了有效实现这一目标，在此准备阶段最好最小化训练样本数量和训练步骤。

### 4.2 推理RL
推理RL阶段使用的查询-验证器对必须满足以下四个标准：（1）它们未在冷启动阶段使用。（2）它们对于冷启动模型是可学习的。（3）它们尽可能具有挑战性。（4）它们涵盖广泛的子领域。我们最终收集了总共3,995个查询-验证器对，并采用GRPO（Shao等人，2024）来更新模型参数。我们观察到，使用大批量大小和每个查询的高卷展次数，以及离线训练以提高样本效率，对训练过程有益。我们还通过控制模型的熵以稳定增加或保持稳定来解决如何平衡探索和利用的问题，这对于维持稳定训练至关重要。结果，在单个RL运行过程中，我们在训练奖励和验证性能上都实现了持续改进，无需对超参数进行任何手动干预。例如，Qwen3-235B-A22B模型的AIME’24分数在总共170个RL训练步骤中从70.1提高到85.1。

### 4.3 思考模式融合
思考模式融合阶段的目标是将“非思考”能力集成到先前开发的“思考”模型中。这种方法允许开发人员管理和控制推理行为，同时还降低了为思考和非思考任务部署单独模型的成本和复杂性。为了实现这一点，我们对推理RL模型进行持续监督微调（SFT），并设计聊天模板以融合两种模式。此外，我们发现能够熟练处理两种模式的模型在不同思考预算下表现一致良好。

#### SFT数据构建
SFT数据集结合了“思考”和“非思考”数据。为了确保第2阶段模型的性能不会因额外的SFT而降低，“思考”数据是通过使用第2阶段模型本身对第1阶段查询进行拒绝采样生成的。另一方面，“非思考”数据经过精心策划，涵盖了广泛的任务，包括编码、数学、指令遵循、多语言任务、创意写作、问答和角色扮演。此外，我们采用自动生成的清单来评估“非思考”数据的响应质量。为了提高低资源语言任务的性能，我们特别增加了翻译任务的比例。

#### 聊天模板设计
为了更好地集成两种模式并使用户能够动态切换模型的思考过程，我们为Qwen3设计了聊天模板，如表9所示。具体而言，对于思考模式和非思考模式的样本，我们分别在用户查询或系统消息中引入/think和/no think标志。这使模型能够遵循用户的输入并选择适当的思考模式。对于非思考模式样本，我们在助手的响应中保留一个空的思考块。这种设计确保了模型内部格式的一致性，并允许开发人员通过在聊天模板中连接一个空的思考块来防止模型进行思考行为。默认情况下，模型以思考模式运行；因此，我们添加了一些用户查询中不包含/think标志的思考模式训练样本。对于更复杂的多轮对话，我们在用户的查询中随机插入多个/think和/no think标志，模型响应遵循遇到的最后一个标志。

|思考模式|非思考模式|
|----|----|
|<|im start|>user {query} /think<|im end|><|im start|>assistant<|FunctionCallBegin|>{thinking content}<|FunctionCallEnd|><|im end|>|<|im start|>user {query} /no think<|im end|><|im start|>assistant<|FunctionCallBegin|>{response}<|im end|>|

#### 思考预算
思考模式融合的另一个优势是，一旦模型学会以非思考和思考模式响应，它自然会发展出处理中间情况的能力——基于不完整的思考生成响应。这种能力为实现对模型思考过程的预算控制奠定了基础。具体而言，当模型的思考长度达到用户定义的阈值时，我们手动停止思考过程并插入停止思考指令：“考虑到用户的时间有限，我现在必须直接根据思考给出解决方案。\n<|FunctionCallBegin|>.\n\n”。插入此指令后，模型将根据其截至该点积累的推理继续生成最终响应。值得注意的是，这种能力不是显式训练的，而是应用思考模式融合的自然结果。

### 4.4 通用RL
通用RL阶段旨在广泛增强模型在各种场景中的能力和稳定性。为了促进这一点，我们建立了一个复杂的奖励系统，涵盖超过20个不同的任务，每个任务都有定制的评分标准。这些任务专门针对以下核心能力的增强：

#### 指令遵循
此能力确保模型准确解释和遵循用户指令，包括与内容、格式、长度和结构化输出使用相关的要求，提供符合用户期望的响应。

#### 格式遵循
除了显式指令外，我们期望模型遵守特定的格式约定。例如，它应该通过在思考和非思考模式之间切换来适当地响应/think和/no think标志，并始终使用指定的token（例如<RichMediaReference>和<|FunctionCallEnd|>）来分隔最终输出中的思考和响应部分。

#### 偏好对齐
对于开放式查询，偏好对齐侧重于提高模型的帮助性、参与度和风格，最终提供更自然和令人满意的用户体验。

#### 代理能力
这涉及训练模型通过指定接口正确调用工具。在RL卷展过程中，允许模型与真实环境执行反馈进行完整的多轮交互循环，从而提高其在长期决策任务中的性能和稳定性。

#### 专业场景能力
在更专业的场景中，我们设计了针对特定上下文的任务。例如，在检索增强生成（RAG）任务中，我们纳入奖励信号以指导模型生成准确且上下文合适的响应，从而最大限度地降低幻觉风险。

为上述任务提供反馈，我们利用了三种不同类型的奖励：

（1）基于规则的奖励：基于规则的奖励已在推理RL阶段广泛使用，对指令遵循（Lambert等人，2024）和格式遵守等一般任务也很有用。精心设计的基于规则的奖励可以高精度地评估模型输出的正确性，防止奖励破解等问题。

（2）带参考答案的基于模型的奖励：在这种方法中，我们为每个查询提供一个参考答案，并提示Qwen2.5-72B-Instruct根据该参考对模型的响应进行评分。这种方法允许更灵活地处理各种任务，而无需严格的格式要求，避免了纯基于规则的奖励可能出现的假阴性。

（3）不带参考答案的基于模型的奖励：利用人类偏好数据，我们训练一个奖励模型为模型响应分配标量分数。这种不依赖参考答案的方法可以处理更广泛的查询，同时有效提高模型的参与度和帮助性。

### 4.5 强到弱蒸馏
强到弱蒸馏管道专门设计用于优化轻量级模型，包括5个密集型模型（Qwen3-0.6B、1.7B、4B、8B和14B）和一个MoE模型（Qwen3-30B-A3B）。这种方法提高了模型性能，同时有效地赋予了强大的模式切换能力。蒸馏过程分为两个主要阶段：

（1）离线蒸馏：在这个初始阶段，我们结合教师模型在/think和/no think模式下生成的输出进行响应蒸馏。这帮助轻量级学生模型发展基本的推理技能和在不同思考模式之间切换的能力，为下一阶段的在线训练奠定坚实基础。

（2）在线蒸馏：在这个阶段，学生模型生成在线序列进行微调。具体而言，对提示进行采样，学生模型以/think或/no think模式生成响应。然后通过将学生模型的logits与教师模型（Qwen3-32B或Qwen3-235B-A22B）的logits对齐来微调学生模型，以最小化KL散度。

### 4.6 后训练评估
为了全面评估指令调整模型的质量，我们采用自动基准来评估模型在思考和非思考模式下的性能。这些基准分为几个维度：

#### 通用任务
我们利用包括MMLU-Redux（Gema等人，2024）、GPQA-Diamond（Rein等人，2023）、C-Eval（Huang等人，2023）和LiveBench（2024-11-25）（White等人，2024）在内的基准。对于GPQA-Diamond，我们对每个查询采样10次并报告平均准确率。

#### 对齐任务
为了评估模型与人类偏好的对齐程度，我们采用了一套专门的基准。对于指令遵循性能，我们报告IFEval（Zhou等人，2023）的严格提示准确率。为了评估一般主题上的人类偏好对齐，我们使用Arena-Hard（Li等人，2024）和AlignBench v1.1（Liu等人，2023b）。对于写作任务，我们依靠Creative Writing V3（Paech，2024）和WritingBench（Wu等人，2025）来评估模型的熟练程度和创造力。

#### 数学与文本推理
为了评估数学和逻辑推理技能，我们采用高级数学基准，包括MATH-500（Lightman等人，2023）、AIME’24和AIME’25（AIME，2025），以及文本推理任务，包括ZebraLogic（Lin等人，2025）和AutoLogi（Zhu等人，2025）。对于AIME问题，每年的问题包括第I部分和第II部分，共30个问题。对于每个问题，我们采样64次并取平均准确率作为最终分数。

#### 代理与编码
为了测试模型在编码和基于代理的任务中的熟练程度，我们使用BFCL v3（Yan等人，2024）、LiveCodeBench（v5，2024.10-2025.02）（Jain等人，2024）和Codeforces Ratings from CodeElo（Quan等人，2025）。对于BFCL，所有Qwen3模型都使用FC格式进行评估，并使用yarn将模型部署到64k的上下文长度以进行多轮评估。一些基线来自BFCL排行榜，取FC和Prompt格式中的较高分数。对于未在排行榜上报告的模型，评估Prompt格式。对于LiveCodeBench，在非思考模式下，我们使用官方推荐的提示，而在思考模式下，我们调整提示模板以允许模型更自由地思考，通过删除限制“除程序外，您不会返回任何内容”。为了评估模型与竞争性编程专家之间的性能差距，我们使用CodeForces计算Elo评级。在我们的基准中，每个问题通过生成多达八次独立的推理尝试来解决。

#### 多语言任务
对于多语言能力，我们评估四种任务：指令遵循、知识、数学和逻辑推理。指令遵循使用Multi-IF（He等人，2024）进行评估，该基准侧重于8种关键语言。知识评估包括两种类型：通过INCLUDE（Romanou等人，2024）评估的区域知识，涵盖44种语言；以及通过MMMLU（OpenAI，2024）评估的一般知识，涵盖14种语言，不包括未优化的约鲁巴语；对于这两个基准，我们仅采样原始数据的10%以提高评估效率。数学任务采用MT-AIME2024（Son等人，2025），涵盖55种语言，以及PolyMath（Wang等人，2025），包括18种语言。逻辑推理由MlogiQA评估，涵盖10种语言，源自Zhang等人（2024）。

对于所有处于思考模式的Qwen3模型，我们使用0.6的采样温度、0.95的top-p值和20的top-k值。此外，对于Creative Writing v3和WritingBench，我们应用1.5的存在惩罚以鼓励生成更多样化的内容。对于处于非思考模式的Qwen3模型，我们将采样超参数配置为temperature=0.7、top-p=0.8、top-k=20和presence penalty=1.5。对于思考和非思考模式，我们将最大输出长度设置为32,768 token，除了AIME’24和AIME’25，我们将此长度扩展到38,912 token以提供足够的思考空间。

| | |OpenAI-o1|DeepSeek-R1|Grok-3-Beta (Think)|Gemini2.5-Pro|Qwen3-235B-A22B|
|----|----|----|----|----|----|----|
|架构|-|MoE|-|-|MoE|
|激活参数数量|-|37B|-|-|22B|
|总参数数量|-|671B|-|-|235B|
|通用任务|||
|MMLU-Redux|92.8|92.9|-|93.7|92.7|
|GPQA-Diamond|78.0|71.5|80.2|84.0|71.1|
|C-Eval|85.5|91.8|-|82.9|89.6|
|LiveBench 2024-11-25|75.7|71.6|-|82.4|77.1|
|对齐任务|||
|IFEval严格提示|92.6|83.3|-|89.5|83.4|
|Arena-Hard|92.1|92.3|-|96.4|95.6|
|AlignBench v1.1|8.86|8.76|-|9.03|8.94|
|Creative Writing v3|81.7|85.5|-|86.0|84.6|
|WritingBench|7.69|7.71|-|8.09|8.03|
|数学与文本推理|||
|MATH-500|96.4|97.3|-|98.8|98.0|
|AIME’24|74.3|79.8|83.9|92.0|85.7|
|AIME’25|79.2|70.0|77.3|86.7|81.5|
|ZebraLogic|81.0|78.7|-|87.4|80.3|
|AutoLogi|79.8|86.1|-|85.4|89.0|
|代理与编码|||
|BFCL v3|67.8|56.9|-|62.9|70.8|
|LiveCodeBench v5|63.9|64.3|70.6|70.4|70.7|
|CodeForces（评级/百分位数）|1891 / 96.7%|2029 / 98.1%|-|2001 / 97.9%|2056 / 98.2%|
|多语言任务|||
|Multi-IF|48.8|67.7|-|77.8|71.9|
|INCLUDE|84.6|82.7|-|85.1|78.7|
|MMMLU 14种语言|88.4|86.4|-|86.9|84.3|
|MT-AIME2024|67.4|73.5|-|76.9|80.8|
|PolyMath|38.9|47.1|-|52.2|54.7|
|MLogiQA|75.5|73.8|-|75.6|77.1|

| | |GPT-4o-2024-11-20|DeepSeek-V3|Qwen2.5-72B-Instruct|LLaMA-4-Maverick|Qwen3-235B-A22B|
|----|----|----|----|----|----|----|
|架构|-|MoE|密集型|MoE|MoE|
|激活参数数量|-|37B|72B|17B|22B|
|总参数数量|-|671B|72B|402B|235B|
|通用任务|||
|MMLU-Redux|87.0|89.1|86.8|91.8|89.2|
|GPQA-Diamond|46.0|59.1|49.0|69.8|62.9|
|C-Eval|75.5|86.5|84.7|83.5|86.1|
|LiveBench 2024-11-25|52.2|60.5|51.4|59.5|62.5|
|对齐任务|||
|IFEval严格提示|86.5|86.1|84.1|86.7|83.2|
|Arena-Hard|85.3|85.5|81.2|82.7|96.1|
|AlignBench v1.1|8.42|8.64|7.89|7.97|8.91|
|Creative Writing v3|81.1|74.0|61.8|61.3|80.4|
|WritingBench|7.11|6.49|7.06|5.46|7.70|
|数学与文本推理|||
|MATH-500|77.2|90.2|83.6|90.6|91.2|
|AIME’24|11.1|39.2|18.9|38.5|40.1|
|AIME’25|7.6|28.8|15.0|15.9|24.7|
|ZebraLogic|27.4|42.1|26.6|40.0|37.7|
|AutoLogi|65.9|76.1|66.1|75.2|83.3|
|代理与编码|||
|BFCL v3|72.5|57.6|63.4|52.9|68.0|
|LiveCodeBench v5|32.7|33.1|30.7|37.2|35.3|
|CodeForces（评级/百分位数）|864 / 35.4%|1134 / 54.1%|859 / 35.0%|712 / 24.3%|1387 / 75.7%|
|多语言任务|||
|Multi-IF|65.6|55.6|65.3|75.5|70.2|
|INCLUDE|78.8|76.7|69.6|80.9|75.6|
|MMMLU 14种语言|80.3|81.1|76.9|82.5|79.8|
|MT-AIME2024|9.2|20.9|12.7|27.0|32.4|
|PolyMath|13.7|20.4|16.9|26.1|27.0|
|MLogiQA|57.4|58.9|59.3|59.9|67.6|

### 4.7 讨论

#### 思考预算的有效性
为了验证Qwen3可以通过增加思考预算来提升其智能水平，我们在数学、编码和STEM领域的四个基准上调整了分配的思考预算。生成的缩放曲线如图2所示，Qwen3展示了与分配的思考预算相关的可扩展和平滑的性能提升。此外，我们观察到，如果我们进一步将输出长度扩展到32K以上，模型的性能预计在未来会进一步提高。我们将此探索留作未来的工作。

#### 在线蒸馏的有效性和效率
我们通过比较蒸馏后的性能和计算成本（以GPU小时衡量）来评估在线蒸馏的有效性和效率，两者都从相同的离线蒸馏8B检查点开始。为了简单起见，我们在此比较中仅关注数学和代码相关的查询。结果总结在表21中，表明蒸馏比强化学习取得了显著更好的性能，同时仅需要约1/10的GPU小时。此外，从教师logits蒸馏使学生模型能够扩展其探索空间并增强其推理潜力，如蒸馏后AIME’24和AIME’25基准上的pass@64分数与初始检查点相比有所提高。相比之下，强化学习并未导致pass@64分数的任何改进。这些观察结果突出了利用更强的教师模型来指导学生模型学习的优势。

|方法|AIME’24|AIME’25|MATH500|LiveCodeBench v5|MMLU-Redux|GPQA-Diamond|GPU小时|
|----|----|----|----|----|----|----|----|
|离线蒸馏|55.0 (90.0)|42.8 (83.3)|92.4|42.0|86.4|55.6|-|
|+强化学习|67.6 (90.0)|55.5 (83.3)|94.8|52.9|86.9|61.3|17,920|
|+在线蒸馏|74.4 (93.3)|65.5 (86.7)|97.0|60.3|88.3|63.3|1,800|

#### 思考模式融合和通用RL的效果
为了评估后训练期间思考模式融合和通用强化学习（RL）的有效性，我们对Qwen-32B模型的各个阶段进行了评估。除了前面提到的数据集，我们还引入了几个内部基准来监控其他能力。这些基准包括：

CounterFactQA：包含反事实问题，模型需要识别问题不真实并避免生成幻觉答案。

LengthCtrl：包括有长度要求的创意写作任务；最终分数基于生成内容长度与目标长度的差异。

ThinkFollow：涉及随机插入/think和/no think标志的多轮对话，以测试模型是否能根据用户查询正确切换思考模式。

ToolUse：评估模型在单轮、多轮和多步工具调用过程中的稳定性。分数包括意图识别的准确性、格式准确性和工具调用过程中的参数准确性。

| |基准|思考阶段2推理RL|阶段3思考模式融合|阶段4通用RL|
|----|----|----|----|----|
| | |思考|非思考|思考|非思考|思考|非思考|
|通用任务|LiveBench 2024-11-25|68.6|70.9 +2.3|57.1|74.9 +4.0|59.8 +2.8|
| |Arena-Hard|86.8|89.4 +2.6|88.5|93.8 +4.4|92.8 +4.3|
| |CounterFactQA\*|50.4|61.3 +10.9|64.3|68.1 +6.8|66.4 +2.1|
|指令与格式遵循|IFEval严格提示|73.0|78.4 +5.4|78.4|85.0 +6.6|83.2 +4.8|
| |Multi-IF|61.4|64.6 +3.2|65.2|73.0 +8.4|70.7 +5.5|
| |LengthCtrl\*|62.6|70.6 +8.0|84.9|73.5 +2.9|87.3 +2.4|
| |ThinkFollow\*|-|88.7|-|98.9 +10.2|-|
|代理|BFCL v3|69.0|68.4 -0.6|61.5|70.3 +1.9|63.0 +1.5|
| |ToolUse\*|63.3|70.4 +7.1|73.2|85.5 +15.1|86.5 +13.3|
|知识与STEM|MMLU-Redux|91.4|91.0 -0.4|86.7|90.9 -0.1|85.7 -1.0|
| |GPQA-Diamond|68.8|69.0 +0.2|50.4|68.4 -0.6|54.6 +4.3|
|数学与编码|AIME’24|83.8|81.9 -1.9|28.5|81.4 -0.5|31.0 +2.5|
| |LiveCodeBench v5|68.4|67.2 -1.2|31.1|65.7 -1.5|31.3 +0.2|

（1）阶段3将非思考模式集成到模型中，该模型在前两个阶段训练后已具备思考能力。ThinkFollow基准得分为88.7，表明模型已发展出初步的模式切换能力，尽管仍偶尔会出错。阶段3还增强了模型在思考模式下的通用和指令遵循能力，CounterFactQA提高了10.9分，LengthCtrl提高了8.0分。

（2）阶段4进一步增强了模型在思考和非思考模式下的通用、指令遵循和代理能力。值得注意的是，ThinkFollow得分提高到98.9，确保了准确的模式切换。

（3）对于知识、STEM、数学和编码任务，思考模式融合和通用RL并未带来显著改进。相比之下，对于AIME’24和LiveCodeBench等具有挑战性的任务，思考模式下的性能在这两个训练阶段后实际上有所下降。我们推测这种退化是由于模型在更广泛的通用任务上训练，这可能损害了其处理复杂问题的专业能力。在Qwen3的开发过程中，我们选择接受这种性能权衡，以增强模型的整体多功能性。

## 5 结论
在本技术报告中，我们介绍了Qwen系列的最新版本Qwen3。Qwen3具有思考模式和非思考模式，使用户能够动态管理用于复杂思考任务的token数量。该模型在包含36万亿token的广泛数据集上进行了预训练，使其能够理解和生成119种语言和方言的文本。通过一系列全面的评估，Qwen3在预训练和后训练模型的各种标准基准上均表现出强大的性能，包括与代码生成、数学、推理和代理相关的任务。

在不久的将来，我们的研究将集中在几个关键领域。我们将继续通过使用质量更高、内容更多样化的数据来扩展预训练规模。同时，我们将致力于改进模型架构和训练方法，以实现有效的压缩、扩展到极长上下文等目标。此外，我们计划增加强化学习的计算资源，特别关注从环境反馈中学习的基于代理的RL系统。这将使我们能够构建能够处理需要推理时间缩放的复杂任务的代理。

## 6 作者
核心贡献者：An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu

贡献者：Bei Chen, Biao Sun, Bin Luo, Bin Zhang, Binghai Wang, Bowen Ping, Boyi Deng, Chang Si, Chaojie Yang, Chen Cheng, Chenfei Wu, Chengpeng Li, Chengyuan Li, Fan Hong, Guobin Zhao, Hang Zhang, Hangrui Hu, Hanyu Zhao, Hao Lin, Hao Xiang, Haoyan Huang, Hongkun Hao, Humen Zhong, Jialin Wang, Jiandong Jiang, Jianqiang Wan, Jianyuan Zeng, Jiawei Chen, Jie Zhang, Jin Xu, Jinkai Wang, Jinyang Zhang, Jinzheng He, Jun Tang, Kai Zhang, Ke Yi, Keming Lu, Keqin Chen, Langshi Chen, Le Jiang, Lei Zhang, Linjuan Wu, Man Yuan, Mingkun Yang, Minmin Sun, Mouxiang Chen, Na Ni, Nuo Chen, Peng Liu, Peng Wang, Peng Zhu, Pengcheng Zhang, Pengfei Wang, Qiaoyu Tang, Qing Fu, Qiuyue Wang, Rong Zhang, Rui Hu, Runji Lin, Shen Huang, Shuai Bai, Shutong Jiang, Sibo Song, Siqi Zhang, Song Chen, Tao He, Ting He, Tingfeng Hui, Wei Ding, Wei Liao, Wei Lin, Wei Zhang, Weijia Xu, Wenbin Ge, Wenmeng Zhou, Wenyuan Yu, Xianyan Jia, Xianzhong Shi, Xiaodong Deng, Xiaoming Huang, Xiaoyuan Li, Ximing Zhou, Xinyao Niu, Xipin Wei, Xuejing Liu, Yang Liu, Yang Yao, Yang Zhang, Yanpeng Li, Yantao Liu, Yidan Zhang, Yikai Zhu, Yiming Wang, Yiwen Hu, Yong Jiang, Yong Li, Yongan Yue, Yu Guan, Yuanzhi Zhu, Yunfei Chu, Yunlong Feng, Yuxin Zhou, Yuxuan Cai, Zeyao Ma, Zhaohai Li, Zheng Li, Zhengyang Tang, Zheren Fu, Zhi Li, Zhibo Yang, Zhifang Guo, Zhipeng Zhang, Zhiying Xu, Zhiyu Yin, Zhongshen Zeng, Zile Qiao, Ziye Meng, Zongmeng Zhang

## A 附录

### A.1 额外评估结果

#### A.1.1 长上下文能力
为了评估长上下文处理能力，我们在RULER基准（Hsieh等人，2024）上报告结果，如表23所示。为了实现长度外推，我们使用YARN（Peng等人，2023），缩放因子=4。在思考模式下，我们将思考预算设置为8192 token，以减轻对极长输入的过度冗长推理。

|Qwen3-4B|85.2 95.1 93.6 91.0 89.1 96.3 96.0 91.8 94.6 98.0 97.8 96.4|87.8 77.8 66.0|
|----|----|----|
|Qwen3-8B|91.2 82.1 96.1 94.0|77.4 85.1|
|Qwen3-14B|93.7 98.4 96.0 96.2 91.6|94.4 91.8 85.6|
|Qwen3-32B|95.0 97.7 97.2 96.4 95.1 93.3 90.6|
|Qwen3-30B-A3B|96.5 97.0 95.3 92.4 89.1 79.2|
|Qwen3-235B-A22B|92.2 95.1 94.8 93.0 92.3 92.0 86.0|
|Qwen3-4B|83.5 92.7 88.7 86.5 83.2 83.0 67.2|
|Qwen3-8B|84.4 94.7 94.4 86.1 80.8 78.3 72.0|
|Qwen3-14B|90.1 95.4 93.6 89.8 91.9 90.6 79.0|
|Qwen3-32B|91.0 94.7 93.7 91.6 92.5 90.0 83.5|
|Qwen3-30B-A3B|86.6 94.1 92.7 89.0 86.6 82.1 75.0|

结果表明：
1. 在非思考模式下，Qwen3在长上下文处理任务中优于类似规模的Qwen2.5模型。
2. 在思考模式下，模型的性能略有下降。我们假设思考内容对这些检索任务没有显著益处，这些任务不依赖推理，反而可能干扰检索过程。我们致力于在未来版本中增强思考模式下的长上下文能力。

#### A.1.2 多语言能力
表24-35呈现了各种语言的详细基准分数，包括西班牙语、法语、葡萄牙语、意大利语、阿拉伯语、日语、韩语、印度尼西亚语、俄语、越南语、德语和泰语。这些表的结果表明，Qwen3系列模型在所有评估基准上均取得了有竞争力的性能，展示了其强大的多语言能力。

为了评估Qwen3在更广泛语言范围内的性能，我们利用Belebele（Bandarkar等人，2023），这是一个用于自然语言理解的基准。我们对该基准支持的80种语言进行了评估，排除了42种未优化的语言，如表36所示（按语系组织）。Qwen3与其他基线模型在Belebele基准上的性能比较如表37所示。结果表明，Qwen3与类似规模的Gemma模型取得了相当的性能，同时显著优于Qwen