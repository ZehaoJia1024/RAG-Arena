# DeepSeek-R1：通过强化学习激发大语言模型的推理能力
## DeepSeek-AI
research@deepseek.com

### 摘要
我们介绍了第一代推理模型DeepSeek-R1-Zero和DeepSeek-R1。DeepSeek-R1-Zero是通过大规模强化学习（RL）训练的模型，无需将监督微调（SFT）作为初步步骤，展现出显著的推理能力。通过强化学习，DeepSeek-R1-Zero自然涌现出许多强大且有趣的推理行为。然而，它面临着可读性差和语言混合等挑战。为解决这些问题并进一步提升推理性能，我们引入了DeepSeek-R1，该模型在强化学习之前融入了多阶段训练和冷启动数据。DeepSeek-R1在推理任务上的性能可与OpenAI-o1-1217相媲美。为支持研究社区，我们开源了DeepSeek-R1-Zero、DeepSeek-R1，以及基于Qwen和Llama从DeepSeek-R1蒸馏得到的六个稠密模型（1.5B、7B、8B、14B、32B、70B）。

|模型|GPQA Diamond（单次通过率）|SWE-bench Verified（已解决）|AIME 2024（单次通过率）|MMLU（百分位）|MATH-500（单次通过率）|Codeforces（单次通过率）|
|----|----|----|----|----|----|----|
|DeepSeek-R1|71.5|72.6|79.8|90.8|97.3|96.3|
|OpenAI-o1-1217|75.7|84.8|79.2|91.8|96.4|96.6|
|DeepSeek-R1-32B|62.1|59.1|79.8|90.8|97.3|96.3|
|OpenAI-o1-mini|60.0|60.0|63.6|85.2|90.0|93.4|
|DeepSeek-V3|49.2|48.9|39.2|88.5|90.2|58.7|

图1 | DeepSeek-R1的基准性能

### 1. 引言
近年来，大语言模型（LLMs）经历了快速迭代和演进（Anthropic, 2024；Google, 2024；OpenAI, 2024a），与通用人工智能（AGI）的差距逐渐缩小。

最近，训练后处理已成为完整训练流程的重要组成部分。研究表明，它能提高推理任务的准确性，与社会价值观保持一致，并适应用户偏好，同时与预训练相比所需的计算资源相对较少。在推理能力方面，OpenAI的o1系列模型（OpenAI, 2024b）首次通过增加思维链（Chain-of-Thought）推理过程的长度引入了推理时缩放。这种方法在数学、编码和科学推理等各种推理任务中取得了显著改进。然而，有效测试时缩放的挑战仍然是研究社区的一个开放问题。先前的几项工作探索了各种方法，包括基于过程的奖励模型（Lightman等人，2023；Uesato等人，2022；Wang等人，2023）、强化学习（Kumar等人，2024）以及蒙特卡洛树搜索和波束搜索等搜索算法（Feng等人，2024；Trinh等人，2024；Xin等人，2024）。然而，这些方法都没有取得与OpenAI的o1系列模型相当的通用推理性能。

在本文中，我们迈出了使用纯强化学习（RL）提高语言模型推理能力的第一步。我们的目标是探索大语言模型在没有任何监督数据的情况下发展推理能力的潜力，专注于通过纯强化学习过程实现自我进化。具体来说，我们使用DeepSeek-V3-Base作为基础模型，并采用GRPO（Shao等人，2024）作为强化学习框架来提高模型在推理任务中的性能。在训练过程中，DeepSeek-R1-Zero自然涌现出许多强大而有趣的推理行为。经过数千次强化学习步骤后，DeepSeek-R1-Zero在推理基准测试中表现出卓越性能。例如，在AIME 2024上的单次通过率从15.6%提高到71.0%，通过多数投票，该分数进一步提高到86.7%，与OpenAI-o1-0912的性能相当。

然而，DeepSeek-R1-Zero面临着可读性差和语言混合等挑战。为解决这些问题并进一步提升推理性能，我们引入了DeepSeek-R1，它结合了少量冷启动数据和多阶段训练流程。具体来说，我们首先收集数千条冷启动数据来微调DeepSeek-V3-Base模型。之后，我们像DeepSeek-R1-Zero一样进行面向推理的强化学习。在强化学习过程接近收敛时，我们通过对强化学习检查点进行拒绝采样来创建新的监督微调数据，并结合来自DeepSeek-V3在写作、事实问答和自我认知等领域的监督数据，然后重新训练DeepSeek-V3-Base模型。用新数据微调后，检查点会经历额外的强化学习过程，同时考虑所有场景的提示。经过这些步骤，我们获得了一个称为DeepSeek-R1的检查点，其性能与OpenAI-o1-1217相当。

我们进一步探索了从DeepSeek-R1到较小稠密模型的蒸馏。使用Qwen2.5-32B（Qwen, 2024b）作为基础模型，直接从DeepSeek-R1进行蒸馏的性能优于在其上应用强化学习。这表明较大基础模型发现的推理模式对于提高推理能力至关重要。我们开源了蒸馏后的Qwen和Llama系列（Dubey等人，2024）。值得注意的是，我们的蒸馏14B模型大幅优于最先进的开源模型QwQ-32B-Preview（Qwen, 2024a），而蒸馏的32B和70B模型在稠密模型的推理基准上创下了新纪录。

### 1.1 贡献
#### 后训练：基础模型上的大规模强化学习
我们直接将强化学习应用于基础模型，而不依赖监督微调（SFT）作为初步步骤。这种方法使模型能够探索解决复杂问题的思维链（CoT），从而开发出DeepSeek-R1-Zero。DeepSeek-R1-Zero展示了自我验证、反思和生成长思维链等能力，为研究社区树立了重要里程碑。值得注意的是，这是首个公开研究，验证了仅通过强化学习就可以激发大语言模型的推理能力，而无需监督微调。这一突破为该领域的未来发展铺平了道路。

我们介绍了开发DeepSeek-R1的流程。该流程包含两个强化学习阶段，旨在发现更好的推理模式并与人类偏好保持一致，以及两个监督微调阶段，作为模型推理和非推理能力的种子。我们相信该流程将通过创建更好的模型使行业受益。

#### 蒸馏：小模型也可以很强大
- 我们证明，较大模型的推理模式可以蒸馏到较小模型中，从而获得比通过小模型上的强化学习发现的推理模式更好的性能。开源的DeepSeek-R1及其API将有助于研究社区在未来蒸馏出更好的小模型。

- 使用DeepSeek-R1生成的推理数据，我们微调了研究社区广泛使用的几个稠密模型。评估结果表明，蒸馏后的较小稠密模型在基准测试中表现出色。DeepSeek-R1-Distill-Qwen-7B在AIME 2024上取得55.5%的成绩，超过了QwQ-32B-Preview。此外，DeepSeek-R1-Distill-Qwen-32B在AIME 2024上得分为72.6%，在MATH-500上得分为94.3%，在LiveCodeBench上得分为57.2%。这些结果大幅优于以前的开源模型，并且与o1-mini相当。我们向社区开源了基于Qwen2.5和Llama3系列的蒸馏1.5B、7B、8B、14B、32B和70B检查点。

### 1.2 评估结果总结
- **推理任务**：（1）DeepSeek-R1在AIME 2024上的单次通过率达到79.8%，略高于OpenAI-o1-1217。在MATH-500上，它取得了令人印象深刻的97.3%的分数，与OpenAI-o1-1217表现相当，并大幅优于其他模型。（2）在与编码相关的任务上，DeepSeek-R1在代码竞赛任务中表现出专家水平，在Codeforces上获得2029的Elo评级，超过了竞赛中96.3%的人类参与者。对于与工程相关的任务，DeepSeek-R1的表现略优于DeepSeek-V3，这可以帮助开发人员完成实际任务。

- **知识**：在MMLU、MMLU-Pro和GPQA Diamond等基准测试中，DeepSeek-R1取得了优异成绩，大幅优于DeepSeek-V3，在MMLU上得分为90.8%，在MMLU-Pro上得分为84.0%，在GPQA Diamond上得分为71.5%。尽管在这些基准测试中其性能略低于OpenAI-o1-1217，但DeepSeek-R1超过了其他闭源模型，展示了其在教育任务中的竞争优势。在事实基准SimpleQA上，DeepSeek-R1优于DeepSeek-V3，展示了其处理基于事实查询的能力。在该基准上观察到类似的趋势，即OpenAI-o1优于4o。

- **其他**：DeepSeek-R1还在广泛的任务中表现出色，包括创意写作、一般问答、编辑、摘要等。它在AlpacaEval 2.0上实现了令人印象深刻的87.6%的长度控制胜率，在ArenaHard上的胜率为92.3%，展示了其智能处理非考试导向查询的强大能力。此外，DeepSeek-R1在需要长上下文理解的任务上表现出色，在长上下文基准上大幅优于DeepSeek-V3。

### 2. 方法
#### 2.1 概述
先前的工作严重依赖大量监督数据来提高模型性能。在本研究中，我们证明即使不使用监督微调（SFT）作为冷启动，通过大规模强化学习（RL）也可以显著提高推理能力。此外，加入少量冷启动数据可以进一步提升性能。在以下部分，我们将介绍：（1）DeepSeek-R1-Zero，它直接将强化学习应用于基础模型，无需任何监督微调数据；（2）DeepSeek-R1，它从使用数千个长思维链（CoT）示例微调的检查点开始应用强化学习；（3）将推理能力从DeepSeek-R1蒸馏到小稠密模型。

#### 2.2 DeepSeek-R1-Zero：基础模型上的强化学习
强化学习在推理任务中已证明具有显著效果，如我们之前的工作（Shao等人，2024；Wang等人，2023）所示。然而，这些工作严重依赖监督数据，收集起来耗时。在本节中，我们探索大语言模型在没有任何监督数据的情况下发展推理能力的潜力，专注于通过纯强化学习过程实现自我进化。我们首先简要概述我们的强化学习算法，然后展示一些令人兴奋的结果，希望能为社区提供有价值的见解。

#### 2.2.1 强化学习算法
为节省强化学习的训练成本，我们采用组相对策略优化（GRPO）（Shao等人，2024），它放弃了通常与策略模型大小相同的评论家模型，而是从组分数中估计基线。具体来说，对于每个问题q，GRPO从旧策略π_θ_old中采样一组输出{o1, o2, ⋯, oG}，然后通过最大化以下目标来优化策略模型π_θ：

\[
\begin{aligned}
\mathcal{J}_{GRPO}(\theta) & =\mathbb{E}\left[q \sim P(Q),\left\{o_{i}\right\}_{i=1}^{G} \sim \pi_{\theta_{old }}(O | q)\right] \\
& \frac{1}{G} \sum_{i=1}^{G}\left(min \left(\frac{\pi_{\theta}\left(o_{i} | q\right)}{\pi_{\theta_{old }}\left(o_{i} | q\right)} A_{i}, clip\left(\frac{\pi_{\theta}\left(o_{i} | q\right)}{\pi_{\theta_{old }}\left(o_{i} | q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_{i}\right)-\beta \mathbb{D}_{K L}\left(\pi_{\theta} \| \pi_{r e f}\right)\right),
\end{aligned}
\]

\[
\mathbb{D}_{K L}\left(\pi_{\theta} \| \pi_{r e f}\right)=\frac{\pi_{r e f}\left(o_{i} | q\right)}{\pi_{\theta}\left(o_{i} | q\right)}-log \frac{\pi_{r e f}\left(o_{i} | q\right)}{\pi_{\theta}\left(o_{i} | q\right)}-1,
\]

其中ε和β是超参数，Ai是优势，使用每组输出对应的一组奖励{r1, r2, ..., rG}计算：

\[
A_{i}=\frac{r_{i}-mean\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)}{std\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)}.
\]

#### 2.2.2 奖励建模
奖励是训练信号的来源，决定了强化学习的优化方向。为训练DeepSeek-R1-Zero，我们采用基于规则的奖励系统，主要由两种奖励组成：

- **准确性奖励**：准确性奖励模型评估响应是否正确。例如，对于具有确定结果的数学问题，要求模型以指定格式（如在框内）提供最终答案，从而能够通过基于规则的方法可靠地验证正确性。同样，对于LeetCode问题，可以使用编译器根据预定义的测试用例生成反馈。

- **格式奖励**：除了准确性奖励模型外，我们还采用格式奖励模型，强制模型将其思维过程放在“<|FunctionCallBegin|>”和“<|FunctionCallEnd|>”标签之间。

在开发DeepSeek-R1-Zero时，我们没有应用结果或过程神经奖励模型，因为我们发现神经奖励模型在大规模强化学习过程中可能会遭受奖励黑客攻击，并且重新训练奖励模型需要额外的训练资源，这会使整个训练流程复杂化。

#### 2.2.3 训练模板
为训练DeepSeek-R1-Zero，我们首先设计一个简单的模板，引导基础模型遵循我们指定的指令。如表1所示，该模板要求DeepSeek-R1-Zero首先生成推理过程，然后是最终答案。我们有意将约束限制在这种结构格式上，避免任何特定于内容的偏见，例如强制反思推理或促进特定的问题解决策略，以确保我们可以在强化学习过程中准确观察模型的自然进展。

#### 2.2.4 DeepSeek-R1-Zero的性能、自我进化过程和顿悟时刻
##### DeepSeek-R1-Zero的性能
图2描绘了DeepSeek-R1-Zero在强化学习训练过程中在AIME 2024基准上的性能轨迹。如图所示，随着强化学习训练的推进，DeepSeek-R1-Zero的性能稳步且持续提升。值得注意的是，AIME 2024上的平均单次通过率显著提高，从最初的15.6%跃升至令人印象深刻的71.0%，达到与OpenAI-o1-0912相当的性能水平。这一显著改进凸显了我们的强化学习算法随着时间推移优化模型性能的有效性。

表2提供了DeepSeek-R1-Zero和OpenAI的o1-0912模型在各种推理相关基准上的比较分析。研究结果表明，强化学习使DeepSeek-R1-Zero能够在不需要任何监督微调数据的情况下获得强大的推理能力。这是一个值得注意的成就，因为它强调了模型仅通过强化学习就能有效学习和泛化的能力。此外，DeepSeek-R1-Zero的性能可以通过应用多数投票进一步增强。例如，在AIME基准上使用多数投票时，DeepSeek-R1-Zero的性能从71.0%提升至86.7%，从而超过了OpenAI-o1-0912的性能。DeepSeek-R1-Zero在有和没有多数投票的情况下都能取得如此有竞争力的性能，凸显了其强大的基础能力及其在推理任务中进一步提升的潜力。

|模型|AIME 2024（单次通过率）|AIME 2024（64次采样共识率）|MATH-500（单次通过率）|GPQA Diamond（单次通过率）|LiveCodeBench（单次通过率）|CodeForces评级|
|----|----|----|----|----|----|----|
|OpenAI-o1-mini|63.6|80.0|90.0|60.0|53.8|1820|
|OpenAI-o1-0912|74.4|83.3|94.8|77.3|63.4|1843|
|DeepSeek-R1-Zero|71.0|86.7|95.9|73.3|50.0|1444|

表2 | DeepSeek-R1-Zero和OpenAI o1模型在推理相关基准上的比较

##### DeepSeek-R1-Zero的自我进化过程
DeepSeek-R1-Zero的自我进化过程是强化学习如何驱动模型自主提高推理能力的引人入胜的展示。通过直接从基础模型启动强化学习，我们可以在不受监督微调阶段影响的情况下密切监控模型的进展。这种方法提供了模型如何随时间演变的清晰视图，特别是在处理复杂推理任务的能力方面。

如图3所示，DeepSeek-R1-Zero的思考时间在整个训练过程中持续改善。这种改进不是外部调整的结果，而是模型内部的内在发展。DeepSeek-R1-Zero通过利用扩展的测试时计算，自然获得了解决日益复杂推理任务的能力。这种计算范围从生成数百到数千个推理token，使模型能够更深入地探索和完善其思维过程。

随着测试时计算的增加，这种自我进化最显著的方面之一是复杂行为的出现。诸如反思（模型重新审视和重新评估其先前步骤）和探索问题解决的替代方法等行为自发出现。这些行为不是显式编程的，而是模型与强化学习环境交互的结果。这种自发发展显著增强了DeepSeek-R1-Zero的推理能力，使其能够以更高的效率和准确性处理更具挑战性的任务。

##### DeepSeek-R1-Zero的顿悟时刻
在DeepSeek-R1-Zero的训练过程中观察到的一个特别有趣的现象是“顿悟时刻”的发生。如表3所示，这一时刻发生在模型的中间版本中。在此阶段，DeepSeek-R1-Zero学会了通过重新评估其初始方法，为问题分配更多的思考时间。这种行为不仅证明了模型不断增长的推理能力，也是强化学习如何导致意外和复杂结果的引人入胜的例子。

这个时刻不仅是模型的“顿悟时刻”，也是观察其行为的研究人员的“顿悟时刻”。它强调了强化学习的力量和美丽：我们不是显式地教模型如何解决问题，而是简单地为它提供正确的激励，它就会自主开发高级的问题解决策略。“顿悟时刻”有力地提醒我们，强化学习有潜力在人工系统中释放新的智能水平，为未来更自主和自适应的模型铺平道路。

**问题**：如果a > 1，那么方程√(a - √(a + x)) = x的实数解之和等于多少？

**响应**：</think>
为了解方程√(a - √(a + x)) = x，我们先将两边平方：
a - √(a + x) = x² ⇒ √(a + x) = a - x²
接下来，我们可以再次平方两边，处理方程：
(a - x²)² = a + x ⇒ a² - 2a x² + (x²)² = a + x ⇒ x⁴ - 2a x² - x + (a² - a) = 0
等等，等等。这里我可以标记一个顿悟时刻。
让我们逐步重新评估，以确定是否可以得到正确的和······我们从方程开始：
√(a - √(a + x)) = x
首先，我们将两边平方：
a - √(a + x) = x² ⇒ √(a + x) = a - x²
接下来，我可以再次平方两边，将方程视为：······

表3 | DeepSeek-R1-Zero中间版本的一个有趣的“顿悟时刻”。模型学会了使用拟人化的语气重新思考。这对我们来说也是一个顿悟时刻，让我们见证了强化学习的力量和美丽。

##### DeepSeek-R1-Zero的缺点
尽管DeepSeek-R1-Zero表现出强大的推理能力，并自主发展出意想不到的强大推理行为，但它面临着几个问题。例如，DeepSeek-R1-Zero面临着可读性差和语言混合等挑战。为了使推理过程更具可读性并与开源社区分享，我们探索了DeepSeek-R1，一种使用对人类友好的冷启动数据的强化学习方法。

#### 2.3 DeepSeek-R1：带冷启动的强化学习
受DeepSeek-R1-Zero有希望的结果的启发，出现了两个自然的问题：1）通过加入少量高质量数据作为冷启动，是否可以进一步提高推理性能或加速收敛？2）我们如何训练一个用户友好的模型，该模型不仅产生清晰连贯的思维链（CoT），而且表现出强大的通用能力？为了解决这些问题，我们设计了一个训练DeepSeek-R1的流程。该流程包括四个阶段，概述如下。

#### 2.3.1 冷启动
与DeepSeek-R1-Zero不同，为了防止从基础模型进行强化学习训练的早期不稳定冷启动阶段，对于DeepSeek-R1，我们构建并收集了少量长思维链数据来微调模型，作为初始强化学习演员。为了收集此类数据，我们探索了几种方法：使用带有长思维链示例的少样本提示，直接提示模型生成带有反思和验证的详细答案，收集可读性格式的DeepSeek-R1-Zero输出，以及通过人类注释者的后处理来完善结果。

在这项工作中，我们收集了数千条冷启动数据来微调DeepSeek-V3-Base，作为强化学习的起点。与DeepSeek-R1-Zero相比，冷启动数据的优势包括：

- **可读性**：DeepSeek-R1-Zero的一个关键限制是其内容通常不适合阅读。响应可能混合多种语言或缺乏Markdown格式来为用户突出显示答案。相比之下，当为DeepSeek-R1创建冷启动数据时，我们设计了一种可读模式，包括在每个响应末尾的摘要，并过滤掉不适合读者的响应。在这里，我们将输出格式定义为|special_token|<推理过程>|special_token|<摘要>，其中推理过程是查询的思维链，摘要用于总结推理结果。

- **潜力**：通过精心设计带有人类先验的冷启动数据模式，我们观察到相对于DeepSeek-R1-Zero更好的性能。我们相信迭代训练是推理模型的更好方法。

#### 2.3.2 面向推理的强化学习
在冷启动数据上微调DeepSeek-V3-Base后，我们应用了与DeepSeek-R1-Zero相同的大规模强化学习训练过程。此阶段专注于增强模型的推理能力，特别是在编码、数学、科学和逻辑推理等推理密集型任务中，这些任务涉及具有明确解决方案的定义良好的问题。在训练过程中，我们观察到思维链经常表现出语言混合，特别是当强化学习提示涉及多种语言时。为了缓解语言混合问题，我们在强化学习训练期间引入了语言一致性奖励，计算为思维链中目标语言单词的比例。尽管消融实验表明这种对齐导致模型性能略有下降，但该奖励与人类偏好一致，使其更具可读性。最后，我们通过直接求和将推理任务的准确性和语言一致性奖励相结合，形成最终奖励。然后，我们在微调后的模型上应用强化学习训练，直到其在推理任务上达到收敛。

#### 2.3.3 拒绝采样和监督微调
当面向推理的强化学习收敛时，我们利用生成的检查点为下一轮收集监督微调（SFT）数据。与主要关注推理的初始冷启动数据不同，此阶段纳入了其他领域的数据，以增强模型在写作、角色扮演和其他通用任务中的能力。具体来说，我们生成数据并按如下所述微调模型：

- **推理数据**：我们策划推理提示，并通过对上述强化学习训练的检查点进行拒绝采样来生成推理轨迹。在先前阶段，我们只包括可以使用基于规则的奖励进行评估的数据。然而，在此阶段，我们通过纳入额外数据来扩展数据集，其中一些数据通过将真实标签和模型预测输入DeepSeek-V3进行判断来使用生成式奖励模型。此外，由于模型输出有时混乱且难以阅读，我们过滤掉了混合语言、长段落和代码块的思维链。对于每个提示，我们采样多个响应并仅保留正确的响应。总共，我们收集了约600k与推理相关的训练样本。

- **非推理数据**：对于非推理数据，如写作、事实问答、自我认知和翻译，我们采用DeepSeek-V3流程并重用DeepSeek-V3的部分监督微调数据集。对于某些非推理任务，我们通过提示调用DeepSeek-V3在回答问题之前生成潜在的思维链。然而，对于更简单的查询，如“你好”，我们在响应中不提供思维链。最后，我们收集了总共约200k与推理无关的训练样本。

我们使用上述策划的约800k样本数据集对DeepSeek-V3-Base进行了两个 epoch 的微调。

#### 2.3.4 全场景强化学习
为了进一步使模型与人类偏好保持一致，我们实施了第二阶段强化学习，旨在提高模型的帮助性和无害性，同时完善其推理能力。具体来说，我们使用奖励信号和多样化的提示分布组合来训练模型。对于推理数据，我们遵循DeepSeek-R1-Zero中概述的方法，利用基于规则的奖励来指导数学、代码和逻辑推理领域的学习过程。对于一般数据，我们求助于奖励模型来捕捉复杂和细微场景中的人类偏好。我们在DeepSeek-V3流程的基础上，采用类似的偏好对和训练提示分布。对于帮助性，我们只关注最终摘要，确保评估强调响应对用户的实用性和相关性，同时最大限度地减少对基础推理过程的干扰。对于无害性，我们评估模型的整个响应，包括推理过程和摘要，以识别和减轻生成过程中可能出现的任何潜在风险、偏见或有害内容。最终，奖励信号和多样化数据分布的集成使我们能够训练一个在推理方面表现出色，同时优先考虑帮助性和无害性的模型。

#### 2.4 蒸馏：赋予小模型推理能力
为了使更高效的小模型具备像DeepSeek-R1一样的推理能力，我们使用DeepSeek-R1策划的800k样本直接微调Qwen（Qwen, 2024b）和Llama（AI@Meta, 2024）等开源模型，如2.3.3节所述。我们的发现表明，这种简单的蒸馏方法显著增强了小模型的推理能力。我们在这里使用的基础模型是Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B和Llama-3.3-70B-Instruct。我们选择Llama-3.3是因为其推理能力略优于Llama-3.1。

对于蒸馏模型，我们仅应用监督微调，不包括强化学习阶段，尽管加入强化学习可以大幅提高模型性能。我们在这里的主要目标是证明蒸馏技术的有效性，将强化学习阶段的探索留给更广泛的研究社区。

### 3. 实验
#### 基准测试
我们在MMLU（Hendrycks等人，2020）、MMLU-Redux（Gema等人，2024）、MMLU-Pro（Wang等人，2024）、C-Eval（Huang等人，2023）、CMMLU（Li等人，2023）、IFEval（Zhou等人，2023）、FRAMES（Krishna等人，2024）、GPQA Diamond（Rein等人，2023）、SimpleQA（OpenAI, 2024c）、C-SimpleQA（He等人，2024）、SWE-Bench Verified（OpenAI, 2024d）、Aider 1、LiveCodeBench（Jain等人，2024）（2024-08 – 2025-01）、Codeforces 2、中国全国高中数学奥林匹克（CNMO 2024）3和美国数学邀请赛2024（AIME 2024）（MAA, 2024）上评估模型。除了标准基准外，我们还使用大语言模型作为评委在开放式生成任务上评估我们的模型。具体来说，我们遵循AlpacaEval 2.0（Dubois等人，2024）和Arena-Hard（Li等人，2024）的原始配置，它们利用GPT-4-Turbo-1106作为评委进行成对比较。在这里，我们只将最终摘要输入评估，以避免长度偏差。对于蒸馏模型，我们报告AIME 2024、MATH-500、GPQA Diamond、Codeforces和LiveCodeBench上的代表性结果。

#### 评估提示
遵循DeepSeek-V3中的设置，MMLU、DROP、GPQA Diamond和SimpleQA等标准基准使用simpleevals框架中的提示进行评估。对于MMLU-Redux，我们在零样本设置中采用Zero-Eval提示格式（Lin, 2024）。就MMLU-Pro、C-Eval和CLUE-WSC而言，由于原始提示是少样本的，我们将提示略微修改为零样本设置。少样本中的思维链可能会损害DeepSeek-R1的性能。其他数据集遵循其原始评估协议和创建者提供的默认提示。对于代码和数学基准，HumanEval-Mul数据集涵盖八种主流编程语言（Python、Java、C++、C#、JavaScript、TypeScript、PHP和Bash）。LiveCodeBench上的模型性能使用思维链格式进行评估，数据收集于2024年8月至2025年1月之间。Codeforces数据集使用10场Div.2比赛的问题以及专家精心设计的测试用例进行评估，之后计算预期评级和竞争对手的百分比。SWE-Bench验证结果通过无代理框架（Xia等人，2024）获得。与AIDER相关的基准使用“diff”格式测量。每个基准的DeepSeek-R1输出限制为最大32,768个token。

#### 基线
我们针对几个强大的基线进行了全面评估，包括DeepSeek-V3、Claude-Sonnet-3.5-1022、GPT-4o-0513、OpenAI-o1-mini和OpenAI-o1-1217。由于在中国大陆访问OpenAI-o1-1217 API具有挑战性，我们根据官方报告报告其性能。对于蒸馏模型，我们还比较了开源模型QwQ-32B-Preview（Qwen, 2024a）。

#### 评估设置
我们将模型的最大生成长度设置为32,768个token。我们发现，使用贪婪解码评估长输出推理模型会导致更高的重复率和不同检查点之间的显著可变性。因此，我们默认使用pass@k评估（Chen等人，2021）并使用非零温度报告pass@1。具体来说，我们使用0.6的采样温度和0.95的top-P值为每个问题生成k个响应（通常在4到64之间，具体取决于测试集大小）。然后，pass@1计算为：

\[pass @ 1=\frac{1}{k} \sum_{i=1}^{k} p_{i}\]

其中pi表示第i个响应的正确性。这种方法提供了更可靠的性能估计。对于AIME 2024，我们还报告使用64个样本的共识（多数投票）结果（Wang等人，2022），表示为cons@64。

#### 3.1 DeepSeek-R1评估

|基准（指标）|Claude-3.5-Sonnet-1022|GPT-4o-0513|DeepSeek-V3|OpenAI-o1-mini|OpenAI-o1-1217|DeepSeek-R1|
|----|----|----|----|----|----|----|
|架构|MoE|MoE| - | - | - | - |
|激活参数数量| - | - | - | - | - | - |
|总参数数量|671B|37B|671B|37B| - | - |
|MMLU（单次通过率）|88.3|87.2|88.5|85.2|91.8|90.8|
|MMLU-Redux（EM）| - | - | - | - | - | - |
|MMLU-Pro（EM）|88.9|78.0|88.0|72.6|89.1|75.9|86.7|80.3|92.9|84.0|
|GPQA Diamond（单次通过率）|88.3|65.0|86.5|84.3|83.7|49.9|59.1|86.1|91.6|83.9|84.8|60.0|90.2|75.7|83.3|92.2|71.5|
|IF-Eval（提示严格）| - | - | - | - | - | - |
|DROP（3-shot F1）| - | - | - | - | - | - |
|SimpleQA（正确）|28.4|38.2|24.9|7.0|47.0|30.1|
|FRAMES（准确率）|72.5|80.5|73.3|76.9|82.5|
|AlpacaEval2.0（LC-胜率）|52.0|51.1|70.0|57.8|87.6|
|ArenaHard（GPT-4-1106）|85.2|80.4|85.5|92.0|92.3|
|英文| - | - | - | - | - | - |
|LiveCodeBench（单次通过率-COT）|38.9|32.9|36.2|53.8|63.4|65.9|
|Codeforces（百分位）|20.3|23.6|58.7|93.4|96.6|96.3|
|Codeforces（评级）|45.3|717|50.8|759|16.0|38.8|42.0|1134|49.6|1820|32.9|41.6|2061|48.9|61.7|49.2|2029|53.3|
|代码| - | - | - | - | - | - |
|数学MATH-500（单次通过率）|78.3|16.0|9.3|74.6|39.2|90.2|90.0|63.6|79.2|96.4|97.3|79.8|
|CNMO 2024（单次通过率）|13.1|10.8|43.2|67.6|78.8|
|中文C-Eval（EM）|76.7|85.4|87.9|76.0|90.9|86.5|89.9|68.9|92.8|91.8|
|CLUEWSC（EM）| - | - | - | - | - | - |
|C-SimpleQA（正确）|55.4|58.7|68.0|40.3|63.7|

表4 | DeepSeek-R1与其他代表性模型的比较

对于MMLU、MMLU-Pro和GPQA Diamond等面向教育的知识基准，DeepSeek-R1与DeepSeek-V3相比表现出卓越性能。这种改进主要归因于STEM相关问题的准确性提高，通过大规模强化学习实现了显著收益。此外，DeepSeek-R1在FRAMES（一个长上下文相关的问答任务）上表现出色，展示了其强大的文档分析能力。这突出了推理模型在AI驱动的搜索和数据分析任务中的潜力。在事实基准SimpleQA上，DeepSeek-R1优于DeepSeek-V3，展示了其处理基于事实查询的能力。在该基准上观察到类似的趋势，即OpenAI-o1优于GPT-4o。然而，DeepSeek-R1在中文SimpleQA基准上的表现比DeepSeek-V3差，主要是由于其在安全强化学习后倾向于拒绝回答某些查询。如果没有安全强化学习，DeepSeek-R1可以达到超过70%的准确率。

DeepSeek-R1在IF-Eval（一个评估模型遵循格式指令能力的基准）上也取得了令人印象深刻的结果。这些改进可以与在监督微调（SFT）和强化学习训练的最后阶段纳入指令遵循数据联系起来。此外，在AlpacaEval2.0和ArenaHard上观察到显著性能，表明DeepSeek-R1在写作任务和开放域问答中的优势。其大幅优于DeepSeek-V3强调了大规模强化学习的泛化优势，这不仅提高了推理能力，还提高了跨不同领域的性能。此外，DeepSeek-R1生成的摘要长度简洁，在ArenaHard上平均为689个token，在AlpacaEval 2.0上为2,218个字符。这表明DeepSeek-R1在基于GPT的评估期间避免引入长度偏差，进一步巩固了其在多个任务中的稳健性。

在数学任务上，DeepSeek-R1表现出与OpenAI-o1-1217相当的性能，大幅优于其他模型。在编码算法任务上观察到类似趋势，如LiveCodeBench和Codeforces，其中面向推理的模型主导这些基准。在工程导向的编码任务上，OpenAI-o1-1217在Aider上优于DeepSeek-R1，但在SWE Verified上取得相当性能。我们相信DeepSeek-R1的工程性能将在后续版本中提高，因为目前相关强化学习训练数据的数量仍然非常有限。

#### 3.2 蒸馏模型评估
如表5所示，简单地蒸馏DeepSeek-R1的输出使高效的DeepSeek-R1-7B（即DeepSeek-R1-Distill-Qwen-7B，以下类似缩写）能够全面优于非推理模型，如GPT-4o-0513。DeepSeek-R1-14B在所有评估指标上都超过了QwQ-32B-Preview，而DeepSeek-R1-32B和DeepSeek-R1-70B在大多数基准上显著超过o1-mini。这些结果展示了蒸馏的强大潜力。此外，我们发现对这些蒸馏模型应用强化学习会产生显著的进一步收益。我们认为这值得进一步探索，因此在此仅呈现简单监督微调蒸馏模型的结果。

|模型|AIME 2024（单次通过率）|AIME 2024（64次采样共识率）|MATH-500（单次通过率）|GPQA Diamond（单次通过率）|LiveCodeBench（单次通过率）|CodeForces评级|
|----|----|----|----|----|----|----|
|GPT-4o-0513|9.3|13.4|74.6|49.9|32.9|759|
|Claude-3.5-Sonnet-1022|16.0|26.7|78.3|65.0|38.9|717|
|OpenAI-o1-mini|63.6|80.0|90.0|60.0|53.8|1820|
|QwQ-32B-Preview|50.0|60.0|90.6|54.5|41.9|1316|
|DeepSeek-R1-Distill-Qwen-1.5B|28.9|52.7|83.9|33.8|16.9|954|
|DeepSeek-R1-Distill-Qwen-7B|55.5|83.3|92.8|49.1|37.6|1189|
|DeepSeek-R1-Distill-Qwen-14B|69.7|80.0|93.9|59.1|53.1|1481|
|DeepSeek-R1-Distill-Qwen-32B|72.6|83.3|94.3|62.1|57.2|1691|
|DeepSeek-R1-Distill-Llama-8B|50.4|80.0|89.1|49.0|39.6|1205|
|DeepSeek-R1-Distill-Llama-70B|70.0|86.7|94.5|65.2|57.5|1633|

表5 | DeepSeek-R1蒸馏模型与其他可比模型在推理相关基准上的比较

### 4. 讨论
#### 4.1 蒸馏与强化学习
在3.2节中，我们可以看到通过蒸馏DeepSeek-R1，小模型可以取得令人印象深刻的结果。然而，仍有一个问题：模型能否通过本文讨论的大规模强化学习训练在不进行蒸馏的情况下取得可比性能？

为了回答这个问题，我们使用数学、代码和STEM数据在Qwen-32B-Base上进行了大规模强化学习训练，训练超过10K步，得到DeepSeek-R1-Zero-Qwen-32B。实验结果如表6所示，表明32B基础模型在大规模强化学习训练后，性能与QwQ-32B-Preview相当。然而，从DeepSeek-R1蒸馏得到的DeepSeek-R1-Distill-Qwen-32B在所有基准上的表现都显著优于DeepSeek-R1-Zero-Qwen-32B。

|模型|AIME 2024（单次通过率）|AIME 2024（64次采样共识率）|MATH-500（单次通过率）|GPQA Diamond（单次通过率）|LiveCodeBench（单次通过率）|
|----|----|----|----|----|----|
|QwQ-32B-Preview|50.0|60.0|90.6|54.5|41.9|
|DeepSeek-R1-Zero-Qwen-32B|47.0|60.0|91.6|55.0|40.2|
|DeepSeek-R1-Distill-Qwen-32B|72.6|83.3|94.3|62.1|57.2|

表6 | 蒸馏模型和强化学习模型在推理相关基准上的比较

因此，我们可以得出两个结论：首先，将更强大的模型蒸馏到较小模型中会产生优异的结果，而依赖本文中提到的大规模强化学习的较小模型需要巨大的计算能力，甚至可能无法达到蒸馏的性能。其次，虽然蒸馏策略既经济又有效，但超越智能的界限可能仍然需要更强大的基础模型和更大规模的强化学习。

#### 4.2 未成功的尝试
在开发DeepSeek-R1的早期阶段，我们也遇到了失败和挫折。我们在此分享我们的失败经验以提供见解，但这并不意味着这些方法无法开发有效的推理模型。

##### 过程奖励模型（PRM）
PRM是一种合理的方法，用于引导模型采用更好的方法解决推理任务（Lightman等人，2023；Uesato等人，2022；Wang等人，2023）。然而，在实践中，PRM有三个主要限制可能阻碍其最终成功。首先，在一般推理中显式定义细粒度步骤具有挑战性。其次，确定当前中间步骤是否正确是一项艰巨的任务。使用模型进行自动注释可能无法产生令人满意的结果，而手动注释不利于扩展。第三，一旦引入基于模型的PRM，不可避免地会导致奖励黑客攻击（Gao等人，2022），并且重新训练奖励模型需要额外的训练资源，这会使整个训练流程复杂化。总之，虽然PRM在对模型生成的前N个响应进行重新排序或协助引导搜索（Snell等人，2024）方面表现出良好的能力，但在我们的实验中，与大规模强化学习过程中引入的额外计算开销相比，其优势有限。

##### 蒙特卡洛树搜索（MCTS）
受AlphaGo（Silver等人，2017b）和AlphaZero（Silver等人，2017a）的启发，我们探索了使用蒙特卡洛树搜索（MCTS）来增强测试时计算的可扩展性。这种方法涉及将答案分解为更小的部分，使模型能够系统地探索解决方案空间。为了促进这一点，我们提示模型生成多个与搜索所需的特定推理步骤相对应的标签。对于训练，我们首先使用收集的提示通过由预训练值模型引导的MCTS找到答案。随后，我们使用生成的问答对来训练演员模型和值模型，迭代完善该过程。

然而，这种方法在扩展训练时遇到了几个挑战。首先，与国际象棋不同，国际象棋的搜索空间相对明确，token生成呈现出指数级更大的搜索空间。为了解决这个问题，我们为每个节点设置了最大扩展限制，但这可能导致模型陷入局部最优。其次，值模型直接影响生成质量，因为它指导搜索过程的每个步骤。训练细粒度的值模型本质上很困难，这使得模型难以迭代改进。虽然AlphaGo的核心成功依赖于训练值模型来逐步提高其性能，但由于token生成的复杂性，这一原则在我们的设置中难以复制。

总之，虽然MCTS与预训练值模型结合使用时可以在推理过程中提高性能，但通过自我搜索迭代提高模型性能仍然是一个重大挑战。

### 5. 结论、局限性和未来工作
在这项工作中，我们分享了我们通过强化学习提高模型推理能力的历程。DeepSeek-R1-Zero代表了一种不依赖冷启动数据的纯强化学习方法，在各种任务中取得了强大的性能。DeepSeek-R1更加强大，利用冷启动数据和迭代强化学习微调。最终，DeepSeek-R1在一系列任务上取得了与OpenAI-o1-1217相当的性能。

我们进一步探索了将推理能力蒸馏到小稠密模型。我们使用DeepSeek-R1作为教师模型生成800K训练样本，并微调了几个小稠密模型。结果很有希望：DeepSeek-R1-Distill-Qwen-1.5B在数学基准上优于GPT-4o和Claude-3.5-Sonnet，在AIME上为28.9%，在MATH上为83.9%。其他稠密模型也取得了令人印象深刻的结果，大幅优于基于相同基础检查点的其他指令微调模型。

未来，我们计划在以下方向对DeepSeek-R1进行研究投资：

- **通用能力**：目前，DeepSeek-R1在函数调用、多轮、复杂角色扮演和JSON输出等任务中的能力落后于DeepSeek-V3。未来，我们计划探索如何利用长思维链来增强这些领域的任务。

- **语言混合**：DeepSeek-R1目前针对中文和英文进行了优化，在处理其他语言的查询时可能会导致语言混合问题。例如，即使查询不是英文或中文，DeepSeek-R1也可能使用英文进行推理和响应。我们计划在未来的更新中解决这一限制。

- **提示工程**：评估DeepSeek-R1时，我们观察到它对提示敏感。少样本提示会持续降低其性能。因此，我们建议用户直接描述问题并使用零样本设置指定输出格式，以获得最佳结果。

- **软件工程任务**：由于评估时间长，影响了强化学习过程的效率，大规模强化学习尚未广泛应用于软件工程任务。因此，DeepSeek-R1在软件工程基准上尚未表现出比DeepSeek-V3的巨大改进。未来版本将通过对软件工程数据进行拒绝采样或在强化学习过程中纳入异步评估来解决这个问题，以提高效率。